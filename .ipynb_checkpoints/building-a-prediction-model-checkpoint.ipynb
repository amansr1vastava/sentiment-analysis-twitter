{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions over amazon recommendation dataset\n",
    "\n",
    "## Predictions\n",
    "The purpose of this analysis is to make up a prediction model where we will be able to predict whether a recommendation is positive or negative. In this analysis, we will not focus on the Score, but only the positive/negative sentiment of the recommendation. \n",
    "\n",
    "To do so, we will work on Amazon's recommendation dataset, we will build a Term-doc incidence matrix using term frequency and inverse document frequency ponderation. When the data is ready, we will load it into predicitve algorithms, mainly naïve Bayesian and regression.\n",
    "\n",
    "In the end, we hope to find a \"best\" model for predicting the recommendation's sentiment.\n",
    "\n",
    "## Loading the data\n",
    "In order to load the data, we will use the SQLITE dataset where we will only fetch the Score and the recommendation summary. \n",
    "\n",
    "As we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"postive\". Otherwise, it will be set to \"negative\". \n",
    "\n",
    "The data will be split into an training set and a test set with a test set ratio of 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "con = sqlite3.connect('../input/database.sqlite')\n",
    "\n",
    "messages = pd.read_sql_query(\"\"\"\n",
    "SELECT Score, Summary\n",
    "FROM Reviews\n",
    "WHERE Score != 3\n",
    "\"\"\", con)\n",
    "\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "Score = messages['Score']\n",
    "Score = Score.map(partition)\n",
    "Summary = messages['Summary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(Summary, Score, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Exploratory analysis\n",
    "\n",
    "After loading the data, here is what it looks like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting our dataset into X_train, X_test, Y_train, Y_test, we have no more integer score, but an appreciation of it : positive or negative :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = messages\n",
    "tmp['Score'] = tmp['Score'].map(partition)\n",
    "print(tmp.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "To format our data and build the Term-doc incidence matrix, many operations will be performed on the data :\n",
    "- Stemming\n",
    "- Stop words removal\n",
    "- Lowering\n",
    "- Tokenization\n",
    "- Pruning (numbers and punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return ' '.join(stems)\n",
    "\n",
    "intab = string.punctuation\n",
    "outtab = \"                                \"\n",
    "trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "#--- Training set\n",
    "\n",
    "corpus = []\n",
    "for text in X_train:\n",
    "    text = text.lower()\n",
    "    text = text.translate(trantab)\n",
    "    text=tokenize(text)\n",
    "    corpus.append(text)\n",
    "        \n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(corpus)        \n",
    "        \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#--- Test set\n",
    "\n",
    "test_set = []\n",
    "for text in X_test:\n",
    "    text = text.lower()\n",
    "    text = text.translate(trantab)\n",
    "    text=tokenize(text)\n",
    "    test_set.append(text)\n",
    "\n",
    "X_new_counts = count_vect.transform(test_set)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "from pandas import *\n",
    "df = DataFrame({'Before': X_train, 'After': corpus})\n",
    "print(df.head(20))\n",
    "\n",
    "prediction = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Multinomial Naïve Bayes learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Multinomial'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bernoulli Naïve Bayes learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Bernoulli'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic regression learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "prediction['Logistic'] = logreg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In order to compare our learning algorithms, let's build the ROC curve. The curve with the highest AUC value will show our \"best\" algorithm.\n",
    "\n",
    "In first data cleaning, stop-words removal has been used, but the results were much worse. Reason for this result could be that when people want to speak about what is or is not good, they use many small words like \"not\" for instance, and these words will typically be tagged as stop-words, and will be removed. This is why in the end, it was decided to keep the stop-words. For those who would like to try it by themselves, I have let the stop-words removal as a comment in the cleaning part of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatt(x):\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    return 1\n",
    "vfunc = np.vectorize(formatt)\n",
    "\n",
    "cmp = 0\n",
    "colors = ['b', 'g', 'y', 'm', 'k']\n",
    "for model, predicted in prediction.items():\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n",
    "    cmp += 1\n",
    "\n",
    "plt.title('Classifiers comparaison with ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the ROC curve, it would appear that the Logistic regression method provides us with the best results, although the AUC value for this method is not outstanding... \n",
    "\n",
    "Let's focus on logistic regression, and vizualise the accuracy, recall and confusion matrix of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(set(Score)))\n",
    "    plt.xticks(tick_marks, set(Score), rotation=45)\n",
    "    plt.yticks(tick_marks, set(Score))\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, prediction['Logistic'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)    \n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
