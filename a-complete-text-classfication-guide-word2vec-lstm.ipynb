{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27bfe1228e31840aee73ac741fa498c8fe227a82"
   },
   "source": [
    "## Text Classification on Amazon Fine Food Dataset with Google Word2Vec Word Embeddings in Gensim and training using LSTM In Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb5f4f667bcea43bba4e4fdebfddcd6c6ac7820f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c884f0eb6264236cecf022353f71feeecd7ae2ae"
   },
   "source": [
    "## [Please star/upvote if u like it.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "727968d2ad72dc35e2315117d90df42ac1fe0fac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19102f2a6d92b2969e4fdc790d260eca4fe6cf4c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89acce557b54ba7cb0ebf85fb3bf22d9e95868a6"
   },
   "source": [
    "### IMPORTING THE MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "739312197e54166212116baab4af1d81fb4e9ccb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u4Qoy1EB5hmJ",
    "outputId": "9fc57d33-b6cc-45c1-c4ec-24394bb61544"
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "% matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "492f57ba52c3349abaedf6044fd952d9fc39ee41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cff40d5497523fa8a1815323f6684c66fc1d008e"
   },
   "source": [
    "### LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1abe90648e9f876e84a5ca74717e373d9877c9ab",
    "colab": {},
    "colab_type": "code",
    "id": "PxRv4O4w6ELq"
   },
   "outputs": [],
   "source": [
    "rev_frame=pd.read_csv(r'../input/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91085adcafd8bc3ecfbd898fe7d3536aea36da49",
    "colab": {},
    "colab_type": "code",
    "id": "RmCtzv5O6UWo"
   },
   "outputs": [],
   "source": [
    "df=rev_frame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7814bbb9e96368171a34663280d4dac827d27a37",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "QiKI5fM664D9",
    "outputId": "2129c76c-53cb-4c43-9a47-15896f90052f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0a9b516821934fff9510beb08e38f854c1204a9"
   },
   "source": [
    "#### A brief description of the dataset from Overview tab on Kaggle : -\n",
    "\n",
    "Data includes:\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e41b160f81537adcb63b344d4786c98eb2874e33",
    "colab": {},
    "colab_type": "code",
    "id": "1hdiPw3eDW6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a92034bd40cb8f1f2e3adf17711001d1d8978da",
    "colab": {},
    "colab_type": "code",
    "id": "3Cqvk7IpDsqI"
   },
   "source": [
    "### DATA CLEANING AND PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8942ed66c74519fe2d4dcb54d144f66e2b45b675"
   },
   "source": [
    "#### Since here I am concerned with sentiment analysis I shall keep only the 'Text' and the 'Score' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6d5d863d0efb7bc29ba4f1a2fb51f1e3917e248",
    "colab": {},
    "colab_type": "code",
    "id": "04Lhx1IhD4MK"
   },
   "outputs": [],
   "source": [
    "df=df[['Text','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9a80c3ae0e318bb9a95071f8aef3205aac25d1e3",
    "colab": {},
    "colab_type": "code",
    "id": "fD_pd159D8Yt"
   },
   "outputs": [],
   "source": [
    "df['review']=df['Text']\n",
    "df['rating']=df['Score']\n",
    "df.drop(['Text','Score'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7e69cc4da9b9b42a1b09556526ad82b0d16a506",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ev-hm43hEZkA",
    "outputId": "324a13fa-1aaf-4ea2-f461-e04a078cfe13"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9c101c03b5f5ab8d89ae92b1b3cef6169d398e6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66f8562443f098c8507486bf3886d3a19dc582e6",
    "colab": {},
    "colab_type": "code",
    "id": "hFLpZZSWEr_q"
   },
   "source": [
    "#### Let us now see if any of the column has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "218cb4ec27d0b4a1509d436f1c26bdb6221e15b7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "b9UiXK5PFTYQ",
    "outputId": "d1700272-b7e5-416c-e5c5-6baee1d2928d"
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "print(df['rating'].isnull().sum())\n",
    "df['review'].isnull().sum()  # no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c9a0e299abc3c1932aa0db184d6044f9a03fafc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7d8261eda8aaf1e28a5952ad7a71e1d906ecc12"
   },
   "source": [
    "#### Note that there is no point for keeping rows with different scores or sentiment for same review text.  So I will keep only one instance and drop the rest of the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab0a87b8287034de77b2c699d19ab1737b9a15b9",
    "colab": {},
    "colab_type": "code",
    "id": "xcccEV8TRfrc"
   },
   "outputs": [],
   "source": [
    "# remove duplicates/ for every duplicate we will keep only one row of that type. \n",
    "df.drop_duplicates(subset=['rating','review'],keep='first',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3b2228a4690d40c3ee93695f23cbf3656280b31",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "IieegGjeRsUx",
    "outputId": "31a4554b-fa6c-4869-b52a-a12b9228a588"
   },
   "outputs": [],
   "source": [
    "# now check the shape. note that shape is reduced which shows that we did has duplicate rows.\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c96efe6b090514b0a6dc4bde185be8b1bb42556a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd6a139e98f2a280753454f5ba81700391d5dc33"
   },
   "source": [
    "#### Let us now print some reviews and see if we can get insights from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a777750ba4765f0bd3087766edbaedf51713490",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "uiFFGnZ28lSs",
    "outputId": "92ac6d5e-6756-4303-944a-b2c24e5330cb"
   },
   "outputs": [],
   "source": [
    "# printing some reviews to see insights.\n",
    "for review in df['review'][:5]:\n",
    "    print(review+'\\n'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2a28a46a39154511eddb1a24938eb5669760668",
    "colab": {},
    "colab_type": "code",
    "id": "aNljFviE93vO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97cd13c873b9c70bb5a0a1b1850563cbabfa78f5"
   },
   "source": [
    "#### There is nothing much that I can figure out except the fact that there are some stray words and some punctuation that we have to remove before moving ahead.\n",
    "\n",
    "**But note that if I remove the punctuation now then it will be difficult to break the reviews into sentences which is required by Word2Vec constructor in Gensim. So we will first break text into sentences and then clean those sentences. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "057596b2c834fcd032d70f7dde839b0d809470e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a718df2ddf66e154aab7cce2607b3af4965c79ec"
   },
   "source": [
    "#### Note that since we are doing sentiment analysis I will convert the values in score column to sentiment. Sentiment is 0 for ratings or scores less than 3 and 1 or  +  elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f34d0ca5176df79971e63d2b283ee890c7743618",
    "colab": {},
    "colab_type": "code",
    "id": "cK5HuI9H_MFf"
   },
   "outputs": [],
   "source": [
    "def mark_sentiment(rating):\n",
    "  if(rating<=3):\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53400cfb2c759e19aa920dfe8c993d7511d46fcd",
    "colab": {},
    "colab_type": "code",
    "id": "yHsAqyByBJsn"
   },
   "outputs": [],
   "source": [
    "df['sentiment']=df['rating'].apply(mark_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd14ee54153c88deaf2d3ad58e3e8178c17662e2",
    "colab": {},
    "colab_type": "code",
    "id": "xkp3SW43Bh1a"
   },
   "outputs": [],
   "source": [
    "df.drop(['rating'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c23230c64d8f3b6c58cead9b0ef295cf6ed0fe7f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "OUYcxtKOBnpL",
    "outputId": "d9a332aa-ff38-4c1f-b9ff-04ad0fcee6a0"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07a40af0e7bee10926af888aa5f21909ff72985d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "eF78FZNuB3Lq",
    "outputId": "969fb056-2f1a-46ec-9bcd-e083e99fb373"
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6c6fe582fcbdb4b6a4cfa2a10c6bc5dcb589210"
   },
   "source": [
    "As you can see the sentiment column now has sentiment of the corressponding product review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9702524be74226327226a9c3da0cb76cd14df388"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4fd194b0f6814612de5a4fce805eb7649450d75",
    "colab_type": "text",
    "id": "xlsuYHoe8wY_"
   },
   "source": [
    "#### Pre-processing steps :\n",
    "\n",
    "1 ) First **removing punctuation and html tags** if any. note that the html tas may be present ast the data must be scraped from net.\n",
    "\n",
    "2) **Tokenize** the reviews into tokens or words .\n",
    "\n",
    "3) Next **remove the stop words and shorter words** as they cause noise.\n",
    "\n",
    "4) **Stem or lemmatize** the words depending on what does better. Herer I have yse lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51b178803da79a6d948d8c5a6aba9d6f7220bacd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8d6aa792cfa36dcc1b33cf90213001891e7429d",
    "colab": {},
    "colab_type": "code",
    "id": "KIm7Erd586HC"
   },
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_reviews(review):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens= review_text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le=WordNetLemmatizer()\n",
    "    stop_words= set(stopwords.words(\"english\"))     \n",
    "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review=\" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c363263f4640a114d2afa17e7e8766878aec090"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "effe8f6f6870d20e292d2c0b91bc2629f213203b",
    "colab": {},
    "colab_type": "code",
    "id": "tzkaC80b96I7"
   },
   "source": [
    "#### Note that pre processing all the reviews is taking way too much time and so I will take only 100K reviews. To balance the class  I have taken equal instances of each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b9d5cde9cac9a93ee404a67bf2e3715ea1a05d4",
    "colab": {},
    "colab_type": "code",
    "id": "EtbatAehCwqU"
   },
   "outputs": [],
   "source": [
    "pos_df=df.loc[df.sentiment==1,:][:50000]\n",
    "neg_df=df.loc[df.sentiment==0,:][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0e23f9b74e34c04e0f43f9fb2fbb34036820eed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Um1-wLvfEKdN",
    "outputId": "50b5fe81-1e89-415e-9f9d-e7f6d6f33f3e"
   },
   "outputs": [],
   "source": [
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e194a611c91fa997d1cd62ccc77d2f7a7ce41bc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "w_-UD-EfDMST",
    "outputId": "26db4b93-5b41-4c7c-8ded-5715b5f92e75"
   },
   "outputs": [],
   "source": [
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "947fd99a414149bae561fc474e3899d795282744"
   },
   "source": [
    "#### We can now combine reviews of each sentiment and shuffle them so that their order doesn't make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c893aaa28c80717a51e7f3d7a36479cf6d63f06",
    "colab": {},
    "colab_type": "code",
    "id": "LmLJwhFPEEDy"
   },
   "outputs": [],
   "source": [
    "#combining\n",
    "df=pd.concat([pos_df,neg_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f98ce122399a51e2c5279b4cde328139205c658f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "hWFZKf3dERLU",
    "outputId": "3f22763a-2dff-47e1-eb86-5e2111d5d447"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46e946e4e593482f373a7ac97cda97b1310cc72a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ngCjsqeLHknP",
    "outputId": "2e53f95a-4cb4-413d-b6f5-5a225e3e9309"
   },
   "outputs": [],
   "source": [
    "# shuffling rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)  # perfectly fine.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fc11a15616e2bee4045ee409e3b4d728eff0849"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2eb88a2676d7cc8c49bdcbf1b48c2c649bde10d2",
    "colab": {},
    "colab_type": "code",
    "id": "TCFFcmQGvauc"
   },
   "source": [
    "### CREATING GOOGLE WORD2VEC WORD EMBEDDINGS IN GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a5f6b6093e1a0179a21424efcc522269551b650"
   },
   "source": [
    "In this section I have actually created the word embeddings in Gensim. Note that I planed touse the pre-trained word embeddings like the google word2vec trained on google news corpusor the famous Stanford Glove embeddings. But as soon as I load the corressponding embeddings through Gensim the runtime dies and kernel crashes ; perhaps because it contains 30L words and which is exceeding the RAM on Google Colab.\n",
    "\n",
    "Because of this ; for now I have created the embeddings by training on my own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2c0d39ac71fca195acb0d3ea5d99b57c625e350",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "b8LYvfQn_GD0",
    "outputId": "2d2b504e-6f63-4f5c-8eab-8dc9b26098a7"
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# # load Google's pre-trained Word2Vec model.\n",
    "# pre_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(r'drive/Colab Notebooks/amazon food reviews/GoogleNews-vectors-negative300.bin', binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79aa54b4ed6a7c2ece95800d7c611e5d13101b32"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d002231f453318a3f418a24ec2b532d063ad5bc4",
    "colab": {},
    "colab_type": "code",
    "id": "i4RbhljSnAVq"
   },
   "source": [
    "#### First we need to break our data into sentences which is requires by the constructor of the Word2Vec class in Gensim. For this I have used Punk English tokenizer from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92fccc63bd1a69dba8e5729a929402c44efd2b76",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eDa2gp9zorg2",
    "outputId": "85c59721-8e9a-40f6-df9a-f80bf3f823bb"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences=[]\n",
    "sum=0\n",
    "for review in df['review']:\n",
    "  sents=tokenizer.tokenize(review.strip())\n",
    "  sum+=len(sents)\n",
    "  for sent in sents:\n",
    "    cleaned_sent=clean_reviews(sent)\n",
    "    sentences.append(cleaned_sent.split()) # can use word_tokenize also.\n",
    "print(sum)\n",
    "print(len(sentences))  # total no of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e42c58b98aeac961923735a17cf0cd5e9400130"
   },
   "source": [
    "#### Now let us print some sentences just to check iff they are in the correct fornat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9818e2ccefffde2fb5f409a13b87e44b7c14b5ca",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "HzJzXDeGqTTR",
    "outputId": "517cb158-a089-4721-fcd0-086eb11ec778"
   },
   "outputs": [],
   "source": [
    "# trying to print few sentences\n",
    "for te in sentences[:5]:\n",
    "  print(te,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ae48acd8cbd00d7376f342bc4d8e514d878c60a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77edb6b38a7da2587bd57cfaf3dd28d8c7dcbf2e"
   },
   "source": [
    "####  Now actually creating the word 2 vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a3b204a1219c074721fff30b1d3912f633eab74",
    "colab": {},
    "colab_type": "code",
    "id": "bbUhUFHCsTx_"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model=gensim.models.Word2Vec(sentences=sentences,size=300,window=10,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2a08433d75209a955eac4dc9fb7d974a6072716"
   },
   "source": [
    "#### Parameters: -\n",
    "\n",
    "**sentences : ** The sentences we have obtained.\n",
    "\n",
    "**size : ** The dimesnions of the vector used to represent each word.\n",
    "\n",
    "**window : ** The number f words around any word to see the context.\n",
    "\n",
    "**min_count : ** The minimum number of times a word should appear for its embedding to be formed or learnt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f04a542e992d3e5bbd4fffd5bd6090b353a4036",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YdpRIli7tfzu",
    "outputId": "d52b2afa-2dab-4403-db68-6d3dfc77f5ac"
   },
   "outputs": [],
   "source": [
    "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84b046217bba8c90bdab37c9ce367bfddc1f7363",
    "colab": {},
    "colab_type": "code",
    "id": "M45arIuQt6nz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11b1f4e6ac115a89ac0307b74829ebe6895ed68d",
    "colab": {},
    "colab_type": "code",
    "id": "Z9ZN6J_Yt9rJ"
   },
   "source": [
    "#### Now can try some things with word2vec embeddings. Thanks to Gensim ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ad6baab82cbba74094017af9e30e138fa278fe7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1309
    },
    "colab_type": "code",
    "id": "9FU9z2LTuBDP",
    "outputId": "0556829c-6110-4348-b233-d2589da93f0e"
   },
   "outputs": [],
   "source": [
    "# embedding of a particular word.\n",
    "w2v_model.wv.get_vector('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0050556ac2df4bba102776191ac0b348a1ef9bde",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LuiNWZrHuOx8",
    "outputId": "81f04fde-f66d-4ad9-df37-ff30855ec44d"
   },
   "outputs": [],
   "source": [
    "# total numberof extracted words.\n",
    "vocab=w2v_model.wv.vocab\n",
    "print(\"The total number of words are : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51ddd393a5cf45056dc8321c85f1ba055d0c1db4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "VIfgI10IuakQ",
    "outputId": "67471d44-d5f7-432f-88c5-40b9ad1483d9"
   },
   "outputs": [],
   "source": [
    "# words most similar to a given word.\n",
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6296cb60a4d4ba8c750bd1de89017122604c38c0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3-IiBwrwvb1g",
    "outputId": "7f59551a-db93-4dc2-a9c0-8770fed03410"
   },
   "outputs": [],
   "source": [
    "# similaraity b/w two words\n",
    "w2v_model.wv.similarity('good','like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f2674b88f53b87ea01c8b817b69c57cc1e18c66"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f83b2a84605e54ae86f90ea86150b8891eb0271a",
    "colab": {},
    "colab_type": "code",
    "id": "KSDerJ9Nv3j7"
   },
   "source": [
    "#### Now creating a dictionary with words in vocab and their embeddings. This will be used when we will be creating embedding matrix (for feeding to keras embedding layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46027121588e43cc98f6e925fc3eedaa561d2802",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xjTUnAL4zcmW",
    "outputId": "b3d4a082-1586-4657-92a9-a6decca455dd"
   },
   "outputs": [],
   "source": [
    "print(\"The no of words :\",len(vocab))\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1dd13a7bb306dd0002a5b51486515220d163c5e",
    "colab": {},
    "colab_type": "code",
    "id": "RKjJClxezPSf"
   },
   "outputs": [],
   "source": [
    "# print(vocab)\n",
    "vocab=list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f09f5f21f612ef5a3b59747218e6c0ccb0ad415",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rcqASqyLwNDk",
    "outputId": "cd2f1c03-5ff9-4f05-9279-81a97c93c24c"
   },
   "outputs": [],
   "source": [
    "word_vec_dict={}\n",
    "for word in vocab:\n",
    "  word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "325ae1b4b37dc7003ddc7e8a1aad7920dfbd5b75",
    "colab": {},
    "colab_type": "code",
    "id": "Gm3rrnKZxgY9"
   },
   "outputs": [],
   "source": [
    "# # just check\n",
    "# for word in vocab[:5]:\n",
    "#   print(word_vec_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65dbc6499c3d912db4efcdacd68fbef5b3ea3008"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3b1b7b10e439cd36ac8bb406d9cf2a3c6b41e5a"
   },
   "source": [
    "### PREPARING THE DATA FOR KERAS EMBEDDING LAYER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c11ae84127697c02d4e6f51d1af3159dacb490ae"
   },
   "source": [
    "Now we have obtained the w2v embeddings. But there are a couple of steps required by Keras embedding layer before we can move on.\n",
    "\n",
    "**Also note that since w2v embeddings have been made now ; we can preprocess our review column by using the function that we saw above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c07b374090e27dac23d28a87fcad917d81b0975c",
    "colab": {},
    "colab_type": "code",
    "id": "aempfGb-TNO3"
   },
   "outputs": [],
   "source": [
    "# cleaning reviews.\n",
    "df['clean_review']=df['review'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a554aeb0450b5d9c1e96418ff3cd545b88814a41"
   },
   "source": [
    "#### We need to find the maximum lenght of any document or review in our case. WE will pad all reviews to have this same length.This will be required by Keras embedding layer. Must check [this](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer) kernel on Kaggle for a wonderful explanation of keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2653074d7010e129645d150ff57a849b1f69cb0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4DmmFNv0TlED",
    "outputId": "4181171a-936b-481a-afce-d9da33d9f831"
   },
   "outputs": [],
   "source": [
    "# number of unique words = 56379.\n",
    "\n",
    "# now since we will have to pad we need to find the maximum lenght of any document.\n",
    "\n",
    "maxi=-1\n",
    "for i,rev in enumerate(df['clean_review']):\n",
    "  tokens=rev.split()\n",
    "  if(len(tokens)>maxi):\n",
    "    maxi=len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "040ec19e7f33e175cfa8a15cd31bd11e48c053db"
   },
   "source": [
    "#### Now we integer encode the words in the reviews using Keras tokenizer. \n",
    "\n",
    "**Note that there two important variables: which are the vocab_size which is the total no of unique words while the second is max_doc_len which is the length of every document after padding. Both of these are required by the Keras embedding layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64fe854ee56f7ac7f1881e690e7d98b5fb10520f",
    "colab": {},
    "colab_type": "code",
    "id": "9pw6qdOfntcS"
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['clean_review'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "865e1a04cd9b78a021b1e1e788af45b244a872cf",
    "colab": {},
    "colab_type": "code",
    "id": "S6K-WwbSnnaa"
   },
   "outputs": [],
   "source": [
    "max_rev_len=1565  # max lenght of a review\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim=300 # embedding dimension as choosen in word2vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "800f7cf8e872c92ff3930eea9c1dd564560225da",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h9NqY0ztof_Z",
    "outputId": "4b56d53b-bcf6-4c7b-ce99-7e4f803c2c86"
   },
   "outputs": [],
   "source": [
    "# now padding to have a amximum length of 1565\n",
    "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46b8fb1d9a1ca780418632cfa7d04bcc79504471"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a6d9bfc59482176a7f5916a7f4ca68aa094e8a8"
   },
   "source": [
    "### CREATING THE EMBEDDING MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4fa0d53b949cb62e6111a2d07245d8f3ed0a8a4c"
   },
   "source": [
    "#### Now we need to pass the w2v word embeddings to the embedding layer in Keras. For this we will create the embedding matrix and pass it as 'embedding_initializer' parameter to the layer.\n",
    "\n",
    "**The embedding matrix will be of dimensions (vocab_size,embed_dim) where the word_index of each word from keras tokenizer is its index into the matrix and the corressponding entry is its w2v vector ;)**\n",
    "\n",
    "**Note that there may be words which will not be present in embeddings learnt by the w2v model. The embedding matrix entry corressponding to those words will be a vector of all zeros.**\n",
    "\n",
    "**Also note that if u are thinkng why won't a word be present then it is bcoz now we have learnt on out own corpus but if we use pre-trained embedding then it may happen that some words specific to our dataset aren't present then in those cases we may use a fixed vector of zeros to denote all those words that earen;t present in th pre-trained embeddings. Also note that it may also happen that some words are not present ifu have filtered some words by setting min_count in w2v constructor.\n",
    "  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f751ae6da742675fc4789fa33421cd87223d651d",
    "colab": {},
    "colab_type": "code",
    "id": "GlJCOsmfxu-S"
   },
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "  embed_vector=word_vec_dict.get(word)\n",
    "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "    embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2053c087cc7c811b4f7cd656ffd078833236632",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1292
    },
    "colab_type": "code",
    "id": "HaH6QTc865Bn",
    "outputId": "c70d670a-8ee6-430b-df04-1dd9d97cf20f"
   },
   "outputs": [],
   "source": [
    "# checking.\n",
    "print(embed_matrix[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de5d6bf92c0ac376d27ddcf1bec33375964b95ef",
    "colab": {},
    "colab_type": "code",
    "id": "b8tBvvHU-Js3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "922f83048b341c34a28b49e6114e612b507b87e6",
    "colab": {},
    "colab_type": "code",
    "id": "rwoFFrW59Rjk"
   },
   "source": [
    "### PREPARING TRAIN AND VALIDATION SETS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff5829cf69c1ac4fdb14c24d9be3c14683363a45",
    "colab": {},
    "colab_type": "code",
    "id": "jNGi31a5AHxC"
   },
   "outputs": [],
   "source": [
    "# prepare train and val sets first\n",
    "Y=keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17b219fa39c7b58d4cc4848e2b8e7ff6d4cd9ae5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1e22e81bc6a49adaad7c4ca4d36e058f6d672f3"
   },
   "source": [
    "### BUILDING A MODEL AND FINALLY PERFORMING TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bfb5c6b91f5bcae59c62b5b6650a8040769cf8b8"
   },
   "source": [
    "Having done all the pre-requisites we finally move onto make model in Keras .\n",
    "\n",
    "**Note that I have commented the LSTM layer as including it causes the trainig loss to be stucked at a value of about 0.6932. I don;t know why ;(.**\n",
    "\n",
    "**In case someone knows please comment below. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b1b2e253f0a76b487e2732ffc5f1b32c5e8e0c0",
    "colab": {},
    "colab_type": "code",
    "id": "ZmsnBmEf-Ktr"
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_rev_len,embeddings_initializer=Constant(embed_matrix)))\n",
    "# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about \n",
    "model.add(Flatten())\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "# model.add(Dense(16,activation='relu'))\n",
    "# model.add(Dropout(0.20))\n",
    "model.add(Dense(2,activation='sigmoid'))  # sigmod for bin. classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b7072ee7d65ff0a6da4099ebc97300ea2595cb7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "915d060b56a109792dfff8a8283e6b94dc483773"
   },
   "source": [
    "#### Let us now print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0bdbc7d142d477bc129b89ab8dc7a606ab07c8b5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "arGSpbbd_0E1",
    "outputId": "ee336fa8-6bc6-4b50-84a6-ccefc544ead9"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "087c686671c54c334bd605a1b76738156f9b975d",
    "colab": {},
    "colab_type": "code",
    "id": "rtVq2ncD_2a1"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed1c7cc4bb2219ca59cf3c777c0222bc2a059083",
    "colab": {},
    "colab_type": "code",
    "id": "xn6g-HknAe9A"
   },
   "outputs": [],
   "source": [
    "# specify batch size and epocj=hs for training.\n",
    "epochs=5\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89e963f56a787c2096706fee9357afcf6121290a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "gvDNMDeWADh9",
    "outputId": "443f2050-c627-4630-b8f4-3324f7e09c4f"
   },
   "outputs": [],
   "source": [
    "# fitting the model.\n",
    "model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37b2ea17080f0d7f04d8da9a09db76333215eb11",
    "colab": {},
    "colab_type": "code",
    "id": "eOsqzdCsD-ia"
   },
   "source": [
    "#### Note that loss as well as val_loss is  is still deceasing. You can train for more no of epochs but I am not so patient ;)\n",
    "\n",
    "**The final accuracy after 5 epochs is about 84% which is pretty decent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e84a04c3f13c1e2b44d64d679999ce9a13eae90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1c2fdb1c54cae3dd9303ed07434da397e0d6d12"
   },
   "source": [
    "### FURTHER IDEAS : -\n",
    "\n",
    "1) ProductId and UserId can be used to track the general ratings of a given product and also to track the review patter of a particular user as if he is strict in reviwing or not.\n",
    " \n",
    "\n",
    "2) Helpfulness feature may tell about the product. This is because gretare the no of people talking about reviews, the mre stronger or critical it is expected to be.\n",
    "\n",
    "3) Summary column can also give a hint.\n",
    "\n",
    "4) One can also try the pre-trained embeddings like Glove word vectors etc...\n",
    "\n",
    "5) Lastly tuning the n/w hyperparameters is always an option;).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0f9b3b3cd7921026230afd18dbcd0a51291585a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7e188d19732570c44e1d86a31059c8ead1d29ea"
   },
   "source": [
    "## THE END!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "806bb1588e3ec46ddf49004f7d13cfb41709b7cd"
   },
   "source": [
    "## [Please star/upvote if it was helpful.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34b9d64d42e6820fe1ab592eda90d58b68ec754f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Amazon fine food reviews(w2v+lstm).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
